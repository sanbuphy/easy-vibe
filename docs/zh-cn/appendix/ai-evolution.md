# 人工智能进化史：从"教它规则"到"让它创造"

> 💡 **学习指南**：本章节通过交互式演示，带你回顾 AI 如何从“只会算数的机器”进化成“能写诗的艺术家”。
>
> 我们将聚焦于三次核心的思维跃迁：从**教它规则**，到**让它模仿**，最终实现**让它创造**。同时，我们也会梳理关键的历史节点，带你理清技术发展的脉络。

<AiEvolutionDemo />

### 关键里程碑 (Timeline)

<AIEvolutionTimelineDemo />

## 0. 引言：机器能思考吗？

1950 年，艾伦·图灵提出了一个问题：**"机器能思考吗？"**

为了回答这个问题，人类尝试了三种截然不同的解法：

1.  **教它规则**（逻辑）：像教小孩一样，把所有规则写给它。
2.  **让它模仿**（概率）：给它看大量数据，让它自己找规律。
3.  **让它创造**（生成）：不仅能分类，还能根据理解创造新东西。

本教程将带你亲手体验这三个阶段。

---

## 1. 符号主义：教机器"守规矩"（20世纪50年代 - 80年代）

早期的 AI 科学家认为：智慧就是**逻辑推理**。
只要我们把世界上的所有知识都写成 `If...Then...` 的规则，机器就能像人一样聪明。

这被称为**专家系统**或**符号主义人工智能**。

### 1.1 什么是"基于规则"？

就像教小孩：

- 如果看到红灯，就停下。
- 如果下雨，就带伞。

在代码中，这表现为：

```javascript
// 基于规则的 AI 示例
function decideTrafficLight(color) {
  if (color === 'red') {
    return 'stop'
  } else if (color === 'yellow') {
    return 'caution'
  } else if (color === 'green') {
    return 'go'
  } else {
    return 'unknown'
  }
}
```

### 1.2 专家系统的巅峰：MYCIN

1970 年代，斯坦福大学开发的 MYCIN 系统能诊断血液感染，准确率达到专家水平。

它的工作原理是：

```lisp
// MYCIN 系统的规则示例 (伪代码)
(IF
  (organism IS gram-positive) AND
  (morphology IS coccus) AND
  (growth-chains IS chains)
THEN
  (identity IS 0.7 streptococcus))
```

_数据示例 (知识库格式)_：

```json
// 专家系统知识库示例
{
  "rules": [
    {
      "id": "RULE-001",
      "conditions": ["traffic_light == red", "speed > 0"],
      "action": "brake",
      "priority": 1
    },
    {
      "id": "RULE-002",
      "conditions": ["weather == rainy", "visibility < 100m"],
      "action": "turn_on_lights",
      "priority": 2
    }
  ]
  // 系统按优先级依次匹配规则，遇到匹配就执行
}
```

### 1.3 交互演示：规则 vs 学习

下方的演示展示了两种方式的区别。

- **左边 (规则)**：你必须显式地写代码 `if (size > 6)`。如果世界变了（比如苹果变小了），你的代码就失效了。
- **右边 (学习)**：你不需要写规则。你只需要给机器看一堆苹果和樱桃的数据，点击 **Train**，它自己会"悟"出一个分界线。

<RuleBasedVsLearningDemo />

### 1.4 符号主义的局限性

规则看起来很完美，但现实世界太复杂了。

<CombinatorialExplosionDemo />

**问题 1：组合爆炸**

- 试图写下"识别猫"的所有规则？不可能！
- "有胡须"？老鼠也有。
- "有尖耳朵"？狗也有。
- "毛茸茸的"？兔子也是。
- 现实世界有无限边界情况，规则永远写不完。

**问题 2：无法处理不确定性**

- 如果规则冲突怎么办？
- 如果遇到没见过的情况怎么办？
- 规则系统很"脆弱"，缺少人类常识。

> ⚠️ **教训**：试图用有限规则描述无限现实，注定失败。这导致了 1980 年代的**AI 寒冬**。

---

## 2. 连接主义：教机器"像人脑一样思考"（21世纪10年代至今）

既然规则写不完，不如换个思路：**让机器自己学**？
科学家开始模仿人脑的结构——**神经元**。

这就是**连接主义**的核心思想。

### 2.1 人脑的启示

人脑有约 860 亿个神经元，每个神经元通过突触连接成千上万个其他神经元。

**关键发现**：

- 单个神经元很"笨"（只是兴奋或不兴奋）
- 但几百亿个神经元连在一起，就产生了智能

### 2.2 感知机

1957 年，康奈尔大学的 Frank Rosenblatt 发明了感知机——这是最简单的人工神经元。

它的工作原理：

1.  **接收输入**：从多个"突触"接收信号（$x_1, x_2, ...$）
2.  **加权求和**：每个输入有对应的**权重**，代表重要性
3.  **激活判断**：如果总和超过某个**阈值（偏置）**，就激活（输出 1）

```text
如果不带公式，怎么理解？

简单来说就是：打分机制。
总分 = (输入1 × 权重1) + (输入2 × 权重2) + ... + 基础分
如果 总分 > 0，输出 1 (激活)
否则，输出 0 (静默)
```

### 2.3 交互演示：玩转神经元

调整下方的**权重**和**偏置**，看看能否控制神经元的输出。

- **权重（$w$）**：代表输入的"重要性"。$w$ 越大，这个输入对结果影响越大。
- **偏置（$b$）**：代表神经元的"门槛"。$b$ 越小，神经元越容易兴奋（输出 1）。

<PerceptronDemo />

### 2.4 从单神经元到深度学习

单个神经元能做什么？只能做简单分类（比如判断"苹果还是樱桃"）。

但如果把神经元分层连接：

```
输入层 (图片像素)
    ↓
隐藏层 1 (识别边缘)
    ↓
隐藏层 2 (识别形状)
    ↓
隐藏层 3 (识别物体部件)
    ↓
输出层 (识别物体)
```

这就是**神经网络**。当网络有很多层时，我们称之为**深度学习**。

<NeuralNetworkVisualizationDemo />

### 2.5 神经网络是如何学习的？

不像专家系统需要人写规则，神经网络通过**看数据**自己学。

**学习过程（反向传播）**：

1.  **前向传播**：输入数据，得到预测结果
2.  **计算误差**：对比预测和真实答案
3.  **反向传播**：根据误差调整每个神经元的权重
4.  **重复**：重复几百万次，直到误差足够小

<BackpropagationDemo />

_数据示例 (训练数据格式)_：

```json
// 图像分类训练数据示例
{
  "dataset": "cats_vs_dogs",
  "samples": [
    {
      "image": "cat_001.jpg",
      "label": 1,  // 1 = 猫
      "features": [0.2, 0.8, 0.5, ...]  // 提取的特征向量
    },
    {
      "image": "dog_001.jpg",
      "label": 0,  // 0 = 狗
      "features": [0.7, 0.3, 0.9, ...]
    }
  ]
  // 神经网络会自动学习：什么样的 feature 组合更可能是猫
}
```

### 2.6 连接主义的突破：2012 年 AlexNet

2012 年，AlexNet 在 ImageNet 竞赛中以压倒性优势夺冠，标志着深度学习时代的到来。

**关键因素**：

- **大数据**：ImageNet 提供了 1400 万张标注图片
- **大算力**：GPU 的并行计算能力让训练深度网络成为可能
- **新算法**：ReLU 激活函数、Dropout 正则化等技术突破

### 2.7 连接主义的局限

深度学习很强大，但也不是完美的：

- **黑盒问题**：虽然能识别猫，但我们说不清"它是怎么识别的"
- **数据饥渴**：需要海量标注数据，获取成本高
- **缺乏常识**：能识别出这是“猫”，但理解不了“猫喜欢抓老鼠”或“猫通常怕狗”这种常识关系（因为它只是在做像素级的统计匹配，而非真正的概念理解）

---

## 3. 生成式人工智能：机器有了"创造力"（21世纪20年代至今）

以前的 AI 主要是**判别式**（这是猫还是狗？）。
现在的 AI 是**生成式**（画一只猫！）。

这一切的背后，是 **Transformer** 架构的诞生。它让 AI 学会了理解上下文，学会了"举一反三"。

### 3.1 从"识别"到"创造"

传统深度学习（判别式模型）：

- 输入：一张图
- 输出：这是猫（概率 98%）

生成式 AI：

- 输入：一句话"一只戴着墨镜的猫"
- 输出：生成一张对应的图片

<DiscriminativeVsGenerativeDemo />

### 3.2 Transformer：AI 的"瑞士军刀"

2017 年，Google 发表论文《Attention Is All You Need》（注意力机制就是你所需的全部），提出 Transformer 架构。

它的核心创新：**注意力机制**

**原理**：让模型在处理一个词时，能"关注"到句子中其他相关的词。

例如："小明把苹果给了**他**的母亲"

当模型处理"他"时，注意力机制会让它关注到"小明"，从而理解"他"指代的是小明。

<AttentionMechanismDemo />

### 3.3 GPT：从文本生成到通用智能

2018 年，OpenAI 发布 GPT-1（生成式预训练变换器）。

**核心思想**：

1.  **预训练**：在海量文本上学习"预测下一个词"
2.  **微调**：在特定任务上调整（比如问答、翻译）

从 GPT-1 (2018) → GPT-2 (2019) → GPT-3 (2020) → GPT-4 (2023)

- 参数量从 1.17 亿 → 1750 亿 → 1.8 万亿（估计）
- 能力从文本生成 → 多模态（图片、音频、视频）

<GPTEvolutionDemo />

### 3.4 生成式人工智能的局限

虽然强大，但也存在问题：

- **幻觉**：一本正经地胡说八道
- **偏见放大**：从训练数据中学到人类偏见
- **不可解释**：仍然是个黑盒，不知道内部怎么运作

---

## 4. AI 范式对比总结

| 时代               | 核心理念        | 代表产物                    | 优势                     | 局限                         |
| :----------------- | :-------------- | :-------------------------- | :----------------------- | :--------------------------- |
| **符号主义**       | 智慧 = 规则     | 深蓝（下棋）、MYCIN（诊断） | 可解释性强，逻辑清晰     | 无法处理模糊、复杂的现实世界 |
| **连接主义**       | 智慧 = 神经网络 | AlphaGo、人脸识别           | 能处理复杂模式，性能强大 | 需要海量数据，是个"黑盒"     |
| **生成式人工智能** | 智慧 = 通用理解 | ChatGPT、Midjourney         | 能创造新内容，理解上下文 | 幻觉、偏见、不可解释         |

**AI 的进化趋势**：

1.  **从人工到自动**：从人写规则 → 机器自动学习
2.  **从单一到通用**：从下棋专用 → 通用人工智能
3.  **从判别到生成**：从分类识别 → 创造新内容

> 关于大语言模型的详细原理，请移步下一章：[大语言模型入门](./llm-intro.md)

---

## 5. 名词速查表

| 名词               | 英文原文                           | 解释                                                                                                |
| :----------------- | :--------------------------------- | :-------------------------------------------------------------------------------------------------- |
| **符号主义**       | Symbolic AI                        | 基于规则的人工智能。认为智能可以用逻辑规则表示。代表：专家系统、深蓝。                              |
| **专家系统**       | Expert Systems                     | 符号主义的代表产物。通过人工编写大量规则来模拟专家决策。代表：MYCIN（医疗诊断）。                   |
| **连接主义**       | Connectionism                      | 基于神经网络的人工智能。模仿人脑神经元连接结构，通过数据自动学习。                                  |
| **感知机**         | Perceptron                         | 最简单的神经网络单元。接收多个输入，加权求和后通过激活函数输出。                                    |
| **神经网络**       | Neural Network                     | 由多个感知机分层连接组成的模型。通过调整权重来学习数据中的模式。                                    |
| **深度学习**       | Deep Learning                      | 使用**多层**神经网络的学习方法。能自动提取层次化特征（边缘 → 形状 → 物体）。                        |
| **反向传播**       | Backpropagation                    | 神经网络的学习算法。通过计算预测误差，反向调整每层的权重，逐步优化模型。                            |
| **生成式人工智能** | Generative AI                      | 能**创造新内容**的人工智能（文本、图片、音频等），而非仅仅是分类或识别。代表：ChatGPT、Midjourney。 |
| **判别式人工智能** | Discriminative AI                  | 用于**分类**的人工智能（如：这是猫还是狗？）。传统深度学习大多是判别式的。                          |
| **Transformer**    | Transformer                        | 2017 年由 Google 提出的架构，基于注意力机制。是现代大语言模型（GPT、BERT）的基础。                  |
| **注意力机制**     | Attention Mechanism                | 让模型在处理一个元素时，能动态"关注"其他相关元素的技术。是 Transformer 的核心。                     |
| **GPT**            | Generative Pre-trained Transformer | OpenAI 的系列模型。通过"预训练 + 微调"范式，在大量文本上学习生成能力。                              |
| **预训练**         | Pre-training                       | 在大规模无标注数据上进行初步训练，学习通用知识（如语言规律）。                                      |
| **微调**           | Fine-tuning                        | 在预训练模型基础上，使用特定任务的小规模数据进行调整，使模型适应具体应用。                          |
| **幻觉**           | Hallucination                      | 生成式人工智能模型"自信地编造错误内容"的现象。如 ChatGPT 编造不存在的论文或事实。                   |
| **通用人工智能**   | Artificial General Intelligence    | 像人类一样具备多领域智能、能自主学习推理的人工智能（尚未实现）。                                    |
